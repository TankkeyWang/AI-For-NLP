{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-02, Probability Model A First Look: An Introduction of Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Review the course online programming code; \n",
    "2. Review the main questions; \n",
    "3. Using wikipedia corpus to build a language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.829 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 今天晚上请你吃大餐，我们一起吃日料 with probility 9.123227846956153e-26\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 9.123227846956153e-26\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 3.379972340324493e-18\n",
      "---- 真是一只好看的小猫 with probility 1.6933652226560576e-15\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 7.714164873681997e-13\n",
      "---- 今晚火锅去吃我 with probility 1.07267876327285e-13\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 8.576046812079107e-11\n",
      "---- 养乐多绿来一杯 with probility 2.7160325500196917e-07\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "df = pd.read_csv('sqlResult_1558435.csv', encoding = 'gb18030')\n",
    "\n",
    "def token(string):\n",
    "    return ''.join(re.findall('[\\w|\\d]+',string))\n",
    "\n",
    "all_articles=df['content'].tolist()\n",
    "\n",
    "all_articles=[token(str(a)) for a in all_articles]\n",
    "\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "\n",
    "text=''\n",
    "for a in all_articles[:10000]:\n",
    "    text += a\n",
    "\n",
    "valid_tokens=[t for t in cut(text) if t != ' ' and t !='n']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "words_count=Counter(valid_tokens)\n",
    "\n",
    "words_count.most_common(10)\n",
    "\n",
    "frequences_all=[f for w,f in words_count.most_common()]\n",
    "frequences_sum=sum(frequences_all)\n",
    "\n",
    "def get_prob(word):\n",
    "    esp=1/frequences_sum\n",
    "    if word in words_count:\n",
    "        return words_count[word]/frequences_sum\n",
    "    else:\n",
    "        return esp\n",
    "    \n",
    "from functools import reduce\n",
    "\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1,n2:n1*n2,numbers)\n",
    "\n",
    "def language_model_one_gram(string):\n",
    "    words=cut(string)\n",
    "    return product([get_prob(w) for w in words])\n",
    "\n",
    "all_2_grams_words=[''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "\n",
    "_2_gram_sum=len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter: return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum\n",
    "\n",
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "def langauge_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability\n",
    "\n",
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = langauge_model_of_2_gram(s1), langauge_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. How to Github and Why do we use Jupyter and Pycharm; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Github是一个代码托管的网站，可以用来控制代码版本以及多人合作，使用git工具可以将Github上的代码下载到本地以及将本地代码上传至Github。\n",
    "Jupyter是一个功能强大的文本编辑器，可以运行Python代码，还可以编辑复杂的数学公式、显示图片等，使用起来十分方便。\n",
    "Pycharm是一个Python的IDE，可以加断点，查看变量的实时值，在调试代码时非常方便，此外还能分析每段代码性能等，在实际开发过程中都是很实用的功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:概率模型是基于概率论和数理统计建立起来的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:拼音输入法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:概率模型的泛化比模板匹配要好。模板匹配的难点在于自然语言很难用若干个模板来表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:语言模型是用来计算一个单词列表是句子的概率的模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  6. Can you came up with some sceneraies at which we could use Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:搜索引擎输入联想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:1-gram模型认为一个单词在一个句子中出现的概率与其他单词出现的概率无关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:优点：简单，计算的参数少，只需要计算每个单词出现的概率即可，可以解决大部分的问题。\n",
    "缺点：不考虑单词之间的关联关系，不够精确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.  What't the 2-gram models; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 2-gram模型认为一个单词在一个句子中出现的概率与他前一个单词相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. what's the web crawler, and can you implement a simple crawler? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。可以用正则表达式实现一个简单的爬虫，在一个给定的网页中抓取有用的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.  There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:难点有：网站有反爬虫机制，有些网页上有脚本无法直接抓取内容等。解决方法有：模拟人浏览网页的操作，使用webkit浏览器内核获取网页内容等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. What't the Regular Expression and how to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:正则表达式是一个有特定规则的字符串，用于过滤满足某些特征的文本。可以借助Python语言中的re包使用正则表达式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: You need to download the corpus from wikipedis:\n",
    "> https://dumps.wikimedia.org/zhwiki/20190401/\n",
    "\n",
    "Step 2: You may need the help of wiki-extractor:\n",
    "\n",
    "> https://github.com/attardi/wikiextractor\n",
    "\n",
    "Step 3: Using the technologies and methods to finish the language model; \n",
    "> \n",
    "\n",
    "Step 4: Try some interested sentence pairs, and check if your model could fit them\n",
    "\n",
    "> \n",
    "\n",
    "Step 5: If we need to solve following problems, how can language model help us? \n",
    "\n",
    "+ Voice Recognization.\n",
    "从每个识别出单词的若干个候选里分别取出一个组成一句话，概率最大的作为识别结果。\n",
    "+ Sogou *pinyin* input.\n",
    "每输入一个音节，将候选的字或词组按出现概率进行排序。\n",
    "+ Auto correction in search engine. \n",
    "与输入法类似，计算所有候选组合的概率，若输入的查询关键字概率低于特定阈值，自动选择概率最大的候选组合。\n",
    "+ Abnormal Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import re\n",
    "from hanziconv import HanziConv\n",
    "from collections import Counter\n",
    "\n",
    "def token(string):\n",
    "    return ''.join(re.findall('[\\w|\\d]+',string))\n",
    "\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "\n",
    "\n",
    "def get_prob(word):\n",
    "    esp = 1 / frequences_sum\n",
    "    if word in words_count:\n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return esp\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)\n",
    "\n",
    "\n",
    "def language_model_one_gram(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])\n",
    "\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter:\n",
    "        return _2_gram_counter[w1 + w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum\n",
    "\n",
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)\n",
    "\n",
    "def langauge_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "\n",
    "    words = cut(sentence)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i - 1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "\n",
    "    return sentence_probability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def traverse_path(dir):\n",
    "    list = os.listdir(dir)  # 列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(dir,list[i])\n",
    "        if os.path.isfile(path):\n",
    "            global all_articles\n",
    "            all_articles+=[HanziConv.toSimplified(r.strip()) for r in open(path,encoding='utf-8').readlines() if not r.startswith('<doc') and not r.startswith('</doc') and r!='\\n']\n",
    "        else:\n",
    "            traverse_path(path)\n",
    "\n",
    "rootdir='text\\AA'\n",
    "\n",
    "all_articles=[]\n",
    "traverse_path(rootdir)\n",
    "text=''\n",
    "print(len(all_articles))\n",
    "for a in all_articles[:100000]:\n",
    "    text += a\n",
    "print('2')\n",
    "valid_tokens=[t for t in cut(text) if t != ' ' and t !='n']\n",
    "words_count=Counter(valid_tokens)\n",
    "\n",
    "frequences_all=[f for w,f in words_count.most_common()]\n",
    "frequences_sum=sum(frequences_all)\n",
    "\n",
    "all_2_grams_words=[''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "\n",
    "_2_gram_sum=len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "\n",
    "need_compared = [\n",
    "    \"今天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = langauge_model_of_2_gram(s1), langauge_model_of_2_gram(s2)\n",
    "\n",
    "    better = s1 if p1 > p2 else s2\n",
    "\n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-' * 4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-' * 4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 优点：泛化性更好，程序代码相对更简洁。缺点：正确率受语料特征影响较大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)  How to solve *OOV* problem?\n",
    "\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this `out-of-vocabulary`(OOV) problems. There are so many intelligent man to solve this probelm. \n",
    "\n",
    "-- \n",
    "\n",
    "The first question is: \n",
    "\n",
    "**Q1: How did you solve this problem in your programming task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 未出现过的词按出现1次来计算概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the sencond question is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task**\n",
    "\n",
    "Reference: \n",
    "+ https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation\n",
    "+ https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf, Page-37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> coding in here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
